{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ff378a-4701-46a8-8add-653944bf16a8",
   "metadata": {},
   "source": [
    "## Fake News Detection Using Machine Learning\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb568c-a1d3-4b42-8100-f9706562e025",
   "metadata": {},
   "source": [
    "### Table of Contents:\n",
    "\n",
    "1. [Task Formulation](#Task-Formulation)\n",
    "2. [Data Overview](#Data-Overview)\n",
    "3. [Data Preprocessing](#Data-Preprocessing)\n",
    "4. [Classification Model](#Classification-Model)\n",
    "5. [Classification Metrics](#Classification-Metrics)\n",
    "6. [Running the Model](#Running-the-Model)\n",
    "7. [Save the Model](#Save-the-Model)\n",
    "8. [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b4bfb-9c9a-4a58-96c4-9418dc7bfdb1",
   "metadata": {},
   "source": [
    "### Task Formulation\n",
    "\n",
    "Fake news's simple meaning is to incorporate information that leads people to the wrong path. Nowadays fake news spreading like water and people share this information without verifying it. This is often done to further or impose certain ideas and is often achieved with political agendas.\n",
    "\n",
    "For media outlets, the ability to attract viewers to their websites is necessary to generate online advertising revenue. So it is necessary to detect fake news.\n",
    "\n",
    "And in this project we'll be doing that  using some __Machine Learning__ and __Natural Language Processing (NLP)__ libraries like __NLTK__, __re__ (Regular Expression), __Scikit Learn__. Let's first import all the neccessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fb71885-c29a-4fe1-8f4f-c15581c7c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pattern\n",
    "from pattern.en import lemma\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6698bb1-4ea1-4336-bb91-7bf1c266b65c",
   "metadata": {},
   "source": [
    "#### *Natural Language Processing*\n",
    "\n",
    "Machine learning model only works with numerical features so we have to convert text data into numerical columns. This kind of text preprocessing is called natural language processing.\n",
    "\n",
    "In-text preprocess we are cleaning our text by steaming, lemmatization, remove stopwords, remove special symbols and numbers, etc. After cleaning the data we have to feed this text data into a vectorizer which will convert this text data into numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2755729f-97e6-4c2c-b67e-8f32615558b0",
   "metadata": {},
   "source": [
    "### Data Overview\n",
    "\n",
    "I downloaded two datasets from [Kaggle]('https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset'). One dataset includes __fake news__ and the other one is for __true news__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae93b7b-992d-4ba8-8ee4-4e2c8de63c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = pd.read_csv('data/fake.csv')\n",
    "fake = pd.read_csv('data/true.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c7259f-f49e-430b-8d2e-f9b113d3225b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23481 entries, 0 to 23480\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    23481 non-null  object\n",
      " 1   text     23481 non-null  object\n",
      " 2   subject  23481 non-null  object\n",
      " 3   date     23481 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 733.9+ KB\n"
     ]
    }
   ],
   "source": [
    "true.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "315cfcd9-d7dd-46a3-be02-b0549c636561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21417 entries, 0 to 21416\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    21417 non-null  object\n",
      " 1   text     21417 non-null  object\n",
      " 2   subject  21417 non-null  object\n",
      " 3   date     21417 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 669.4+ KB\n"
     ]
    }
   ],
   "source": [
    "fake.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68749733-6f8c-4866-86a6-4ee484bda01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bd9db97-c304-4dce-b825-067d95d2d871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba27e5f-f151-4ee3-b5c0-fa049aca0c42",
   "metadata": {},
   "source": [
    "There are __23481__ entries for __true news__ dataset and __21417__ - in __fake news__ dataset. Both datasets include the following information - __title, subject, date__ and the news __text__ itself. There are __no missing  values__ in both sets, so no need to __impute__ any value.\n",
    "\n",
    "Firstly, we need to create in each dataset a column labeled  with 1 and 0 indicating __fake__ (1) and __true__ (0) news and then __combine__ both datasets in one final set. \n",
    "\n",
    "Our __final dataset__ would be __balanced__ because both categories approximate same number of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bc9306c-1dcd-4494-8139-afec2d95dd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21417, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0906b2ed-3473-46d5-8192-ed5d79017966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23481, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2f2819d-968a-4d96-b0a8-bdc2abf1d286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22958</th>\n",
       "      <td>Trump and his team made a great deal of the ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13148</th>\n",
       "      <td>CAIRO (Reuters) - The Arab League on Tuesday c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23066</th>\n",
       "      <td>A student at Transylvania University was injur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41358</th>\n",
       "      <td>Not a bad imitation of a black pastor from a g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12592</th>\n",
       "      <td>(Reuters) - Reuters photographers witnessed th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4163</th>\n",
       "      <td>SAN ANTONIO, Texas (Reuters) - A special feder...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28874</th>\n",
       "      <td>With only three clowns left in GOP clown car, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33074</th>\n",
       "      <td>Mark Steyn was on fire last night! If you were...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20960</th>\n",
       "      <td>WASHINGTON (Reuters) - U.S. Ambassador to the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42792</th>\n",
       "      <td>The Hoke County School school system defended ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "22958  Trump and his team made a great deal of the ca...      0\n",
       "13148  CAIRO (Reuters) - The Arab League on Tuesday c...      1\n",
       "23066  A student at Transylvania University was injur...      0\n",
       "41358  Not a bad imitation of a black pastor from a g...      0\n",
       "12592  (Reuters) - Reuters photographers witnessed th...      1\n",
       "4163   SAN ANTONIO, Texas (Reuters) - A special feder...      1\n",
       "28874  With only three clowns left in GOP clown car, ...      0\n",
       "33074  Mark Steyn was on fire last night! If you were...      0\n",
       "20960  WASHINGTON (Reuters) - U.S. Ambassador to the ...      1\n",
       "42792  The Hoke County School school system defended ...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake['label'] = 1\n",
    "true['label'] = 0\n",
    "df = pd.merge(fake[['text', 'label']], true[['text', 'label']], how='outer')\n",
    "\n",
    "# shuffle\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c4c6a2-b544-412a-987e-a53f64f1de80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23481\n",
       "1    21417\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffb9f8b-0460-4e87-bb4b-d5d65fdd8754",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "#### Cleaning Data\n",
    "\n",
    "If we use text data directly without cleaning then it is very hard for the Machine Learning algorithm to detect patterns in that text. Moreover, it can generate errors. So, we need first to clean text data - remove some unusable words, special symbols etc. \n",
    "\n",
    "I will create a separate function to clean the data and then run it on our data. It will __lemmatize__ the text - convert each word in its __base form__, remove __stopwords__, __numbers__ and __special symbols__.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02ab82ef-7565-473f-9929-0817229d0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # convert text to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # replace numbers and special symbols with a space\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    \n",
    "    # make tokens from splitted data\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # remove stopwords and lemmatization \n",
    "    cleaned_text = [lemma(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # join token with space\n",
    "    cleaned_text = ' '.join(cleaned_text)\n",
    "    \n",
    "    # return final clean text\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1c75c1e-86a8-4704-b670-2dd8da67ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eb5328b-0234-4b77-80fc-b4b614b09b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22958</th>\n",
       "      <td>trump team make great deal campaign hillary cl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13148</th>\n",
       "      <td>cairo reuter arab league tuesday condemn kill ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23066</th>\n",
       "      <td>student transylvania university injure attack ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41358</th>\n",
       "      <td>bad imitation black pastor guy quit go church ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12592</th>\n",
       "      <td>reuter reuter photographer witness biggest sto...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "22958  trump team make great deal campaign hillary cl...      0\n",
       "13148  cairo reuter arab league tuesday condemn kill ...      1\n",
       "23066  student transylvania university injure attack ...      0\n",
       "41358  bad imitation black pastor guy quit go church ...      0\n",
       "12592  reuter reuter photographer witness biggest sto...      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ecce62-4243-4b9c-82e5-03cea69047d6",
   "metadata": {},
   "source": [
    "#### Split Data\n",
    "\n",
    "This is an important step. We are going to train the machine learning model on the training dataset and then test the model on the testing set. \n",
    "\n",
    "Our target `y` will be a __label__ column, and the __text__ column is the feature in our dataset - `X`.\n",
    "\n",
    "Then we split our main dataset into `x_train`, `y_train`, `x_test` and `y_test` using `train_test_split` function from the __Scikit learn__ library.\n",
    "\n",
    "We split our 80% data for the training set and the remaining 20% data for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98bfbe15-0dc7-4755-841b-dc233122d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb37ff-9849-4a49-87fc-1052a7c7f940",
   "metadata": {},
   "source": [
    "#### Tfidf Vectorizer\n",
    "***Tfidf-Vectorizer***: *Term Frequency * Inverse Document Frequency*\n",
    "\n",
    "***Term Frequency***: The number of times word appear in the text divided by the total number of words there:\n",
    "$$ tf_{i,j} = \\frac{n_{i,j}}{\\sum_k{n_{n,j}}}$$\n",
    "***Inverse Document Frequency***: a measure of how much information the word provides, i.e., if it is common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by taking the log of the total number of documents divided by the number of documents containing the term):\n",
    "$$ idf(w) = \\log(\\frac{N}{df_t}) $$\n",
    "\n",
    "So, Tfidf vectorizer is:\n",
    "$$w_{i,j} = tf_{i,j} \\times \\log(\\frac{N}{df_i})$$\n",
    "\n",
    "__Tfidf Vectorizer__ is already in __Scikit Learn__ library, so we can take it, fit on our training dataset and transform its values on both train and test datasets with respect to the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55eeae1a-e069-4c07-bc50-611dedacbf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=50000, lowercase=False, ngram_range=(1,2))\n",
    "\n",
    "vec_train_data = vectorizer.fit_transform(x_train)\n",
    "vec_test_data = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ee584-1e21-41c8-a455-a747ab612b48",
   "metadata": {},
   "source": [
    "After vectorizing the data it will return the sparse matrix so that for machine learning algorithms we have to convert it into arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c68e049c-83c5-47de-88a2-8aeddeb77398",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_train_data = vec_train_data.toarray()\n",
    "vec_test_data = vec_test_data.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be91f4c9-2aeb-4a0f-89e1-b8f8e1768cfb",
   "metadata": {},
   "source": [
    "### Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15772b-a719-4589-87b3-e42110ea2404",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Multinomial Naive Bayes Classifier\n",
    "\n",
    "***Naive Bayes***: the Naive Bayes Classifier is based on the Bayesian theorem and is particularly suited with high dimensional data.\n",
    "\n",
    "$$ P(A | B) = \\frac{P(B |A)\\cdot P(A)}{P(B)} $$\n",
    "where:\n",
    " * $A,B$ - events\n",
    " * $P(A|B)$ - probability of $A$ given $B$ is true\n",
    " * $P(B|A)$ - probability of $B$ given $A$ is true\n",
    " * $P(A), P(B)$ - independent probabilities of $A$ and $B$\n",
    "\n",
    "\n",
    "\n",
    "***Multinomial Naive Bayes*** is used for classification of discrete data. It is very useful in text processing. The text will be converted to a vector of word count. \n",
    "\n",
    "This technique is predefined in __Scikit Learn__ Library. So we can import that class and create an object of Multinomial Naive Bayes Class.\n",
    "\n",
    "1. Fit the classifier on our vectorized train data.\n",
    "2. Use the predict method to predict the result on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62f2a8f6-abf5-4269-8fc4-6608d0fef72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35918, 50000), (8980, 50000))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_train_data.shape , vec_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "342c4acf-d16f-4cd1-a4be-164e532b43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.DataFrame(vec_train_data , columns=vectorizer.get_feature_names_out())\n",
    "testing_data = pd.DataFrame(vec_test_data , columns= vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50248e19-03ea-4734-9689-13f57263f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(training_data, y_train)\n",
    "y_pred  = clf.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b759083-876e-4dd6-be0e-fde0f14e5923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44db113-d7ef-4a78-80fb-3ff443dd9e58",
   "metadata": {},
   "source": [
    "### Classification Metrics\n",
    "\n",
    "To check how good is our model we use some metrics to find the accuracy of our model on both train and test sets. There are many types of classification metrics available in Scikit learn, we'll use some of them:\n",
    "\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "- Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "990c599a-a474-4dab-b03f-c14eb911574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      4641\n",
      "           1       0.95      0.94      0.95      4339\n",
      "\n",
      "    accuracy                           0.95      8980\n",
      "   macro avg       0.95      0.95      0.95      8980\n",
      "weighted avg       0.95      0.95      0.95      8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test , y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "296f2130-d523-4798-9c06-1c647d392f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96     18840\n",
      "           1       0.96      0.95      0.95     17078\n",
      "\n",
      "    accuracy                           0.96     35918\n",
      "   macro avg       0.96      0.96      0.96     35918\n",
      "weighted avg       0.96      0.96      0.96     35918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = clf.predict(training_data)\n",
    "print(classification_report(y_train , y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "827e6f4e-33c0-4705-ba91-bd0bfa8fe31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9572080850826884"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on train data\n",
    "accuracy_score(y_train , y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88916b4f-be75-4672-a997-df6e89491a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.948663697104677"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on test data\n",
    "accuracy_score(y_test , y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62547ea7-6db8-464c-bec3-345c288575e6",
   "metadata": {},
   "source": [
    "Scores on both train and test datasets are very high. Thus, our model performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84cb371-24ec-47ce-af4c-529d67702a8c",
   "metadata": {},
   "source": [
    "### Running the Model\n",
    "\n",
    "We have a __line of text__ that I took from the __news website__. Let's check if it is __fake__ or __true__. As it was done before with the __train data__, I'm going to __clean__ this text line, __lemmatize__, __vectorize__ and then put this line into the __model__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad2e2c4f-6771-453c-bec3-9e08e0545f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"Ukraine's most nationalist region was once a hotbed of pro-Russian sentiment â€“ how and why did it change?\"\n",
    "news = clean_text(line)\n",
    "vec_news = vectorizer.transform([news]).toarray()\n",
    "vec_news_data = pd.DataFrame(vec_news, columns= vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3669bc3d-f169-42dc-8ac8-df778c46d8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake News!\n"
     ]
    }
   ],
   "source": [
    "single_prediction = clf.predict(vec_news_data)\n",
    "\n",
    "print('Fake News!' if single_prediction[0] else 'True News')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1cdb04-ab74-47c9-b99e-2046924c3153",
   "metadata": {},
   "source": [
    "Recall that __1__ is fake and __0__ is true. So, the news line I found is __Fake__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc770301-48b9-4b80-905e-30d0a362faec",
   "metadata": {},
   "source": [
    "### Save the Model\n",
    "\n",
    "Now we can save the model to have a possibility to use it again without double training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c66858f-73e2-4ce8-9bca-ed4a3681d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, 'data/model.pkl')\n",
    "model = joblib.load('data/model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cb1b6-2867-40f1-b4c1-7b2aae771dbe",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "With this ML model we can detect __fake news__. We took __Fake__ and __True News__ datasets, implemented a __Text cleaning__ function, __TfidfVectorizer__, initialized __Multinomial Naive Bayes Classifier__, and fit our model. We ended up obtaining an __accuracy of 94.87%__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5991dd96-c2bb-4f72-bdc3-442fbba0ae4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
